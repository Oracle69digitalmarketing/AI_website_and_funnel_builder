version: "3.9"

services:
  llama-api:
    build: .
    container_name: llama-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    command: bash start.sh

  # Optional: NGINX reverse proxy (frontend access via port 80)
  nginx:
    image: nginx:alpine
    container_name: llama-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - llama-api
